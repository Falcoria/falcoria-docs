{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Falcoria","text":"<p>Falcoria is a system for team-based port scanning on large scopes. It maintains a shared scope state that every scan updates \u2014 no separate files to merge.</p>"},{"location":"#the-problem","title":"The problem","text":"<p>On a large engagement, scan output multiplies fast. Different people use different tools, the scope gets split into chunks, rescans add more files. After a few days, there's no single place to check what's open \u2014 the answer is spread across dozens of files from different people and points in time.</p> <p>Teams deal with this using whatever is at hand \u2014 shared folders, spreadsheets, Notion, Slack. None of these are built for scan data, and the results still have to be merged.</p> <p></p> <p>Traditional: scan \u2192 file \u2192 merge \u2192 scope state. Falcoria: scan \u2192 scope state.</p>"},{"location":"#how-falcoria-approaches-this","title":"How Falcoria approaches this","text":"<p>Falcoria treats scans as state updates. Each scan \u2014 whether Falcoria's own or an imported report \u2014 writes directly into a shared dataset called ScanLedger. There is no intermediate file step.</p>"},{"location":"#what-this-gives-you","title":"What this gives you","text":"<ul> <li> <p>Shared scope state \u2014 each scan updates a persistent shared dataset. No intermediate files or manual merge steps \u2014 every team member sees the same current scope state immediately after a scan completes.</p> </li> <li> <p>Import modes \u2014 operator controls how each scan is applied \u2014 HTTP-only first, then full range, then service detection \u2014 each pass adds to the same result without overwriting previous data. The system parses scan commands to know which ports were in scope \u2014 ports in range but absent from results are marked as closed.</p> </li> <li> <p>Change tracking \u2014 records state transitions between scans: newly opened ports, closures, service changes. No manual diffing of scan outputs.</p> </li> <li> <p>Deduplication \u2014 eliminates redundant targets before scanning starts. Different team members can submit overlapping targets \u2014 each is scanned only once. Deduplication cuts the scope by ~20% to nearly 50% before a single packet is sent. Fewer duplicate probes, lower network noise, reduced risk of triggering rate limits.</p> </li> <li> <p>Distributed execution \u2014 splits work across multiple workers on separate machines and network paths. Ten workers scan roughly ten times faster than one, without increasing the rate on any single target. See Benchmarks for measured results.</p> </li> <li> <p>Resumable scans \u2014 interrupted scans pick up where they left off. The task queue tracks completed units of work.</p> </li> <li> <p>Export \u2014 the current scope state can be generated as Nmap XML or JSON at any time, preserving compatibility with existing tooling.</p> </li> </ul> <p></p> <p>Scope state evolving across three scans: ports are added, services detected, changes tracked.</p>"},{"location":"#who-it-is-for","title":"Who it is for","text":"<ul> <li>Penetration testers managing large scopes with a team</li> <li>Red team operators running repeated discovery across changing networks</li> <li>Security engineers maintaining current host/port/service visibility</li> <li>Security automation teams looking for an API-driven backend to aggregate scan data from various sources</li> </ul>"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li>Getting Started \u2014 setup and first scan</li> <li>Architecture \u2014 how the system is structured</li> <li>Concepts \u2014 how the pipeline works end to end</li> <li>Workflows \u2014 common usage patterns</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>Falcoria has two subsystems: Scan Execution and Data Aggregation. They communicate through APIs and can run independently.</p> <p></p>"},{"location":"architecture/#data-aggregation","title":"Data Aggregation","text":"<p>The center of the system is ScanLedger \u2014 the shared dataset where all scan results end up. Data is organized by project. Each project maintains its own shared state with unique records per IP, port, and hostname. Scans, imports, exports \u2014 everything is scoped to a project. When new scan data comes in, it gets merged into existing records according to import mode rules.</p> <p>ScanLedger also tracks change history \u2014 if a port state, service, or banner changes between scans, the change is recorded.</p> <p>ScanLedger can be used on its own, without the scan execution subsystem. If you already run your own scans with Nmap or other tools, you can import reports into ScanLedger directly and use it purely for data aggregation.</p>"},{"location":"architecture/#scan-execution","title":"Scan Execution","text":"<p>Handles everything from accepting a scan request to delivering results to ScanLedger. Three components:</p> <p>Tasker accepts scan requests via API or <code>falcli</code>. It prepares targets before anything gets scanned: expands CIDRs, resolves hostnames, removes duplicates, and checks what's already been scanned or queued. The output is a set of discrete scan tasks, each targeting a single IP address with a defined port range and associated hostnames kept as metadata.</p> <p>Queue holds prepared tasks until workers pick them up. If the same target is already queued, the duplicate is rejected.</p> <p>Workers pull tasks from the queue and execute scans. Each worker runs on its own machine with its own network path and IP address. Results go straight to ScanLedger via API. Workers don't talk to each other \u2014 the queue handles assignment, ScanLedger handles merging.</p> <p>Adding workers scales throughput linearly. Ten workers finish roughly ten times faster than one, until target-side rate limits or network saturation become the bottleneck.</p>"},{"location":"architecture/#data-flow","title":"Data flow","text":"<ol> <li>User submits targets and a scan config to Tasker (via <code>falcli</code> or API)</li> <li>Tasker deduplicates targets and creates scan tasks</li> <li>Tasks enter the Queue</li> <li>Workers pick up tasks and execute scans</li> <li>Workers send results to ScanLedger via API</li> <li>ScanLedger merges results into the shared state</li> <li>Team queries current data via <code>falcli</code>, API, or export (Nmap XML, JSON)</li> </ol>"},{"location":"architecture/#deployment","title":"Deployment","text":"<p>Tasker and ScanLedger are FastAPI applications. Each exposes API docs at <code>/docs</code> when running.</p> <p>Workers can be deployed anywhere \u2014 cloud VMs, VPSes, VPN endpoints. The only requirement is network access to the queue and to ScanLedger. The number of workers determines how fast the scope gets covered.</p> <p>For data aggregation only (no scanning), ScanLedger and <code>falcli</code> are enough.</p>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>Performance data for open port discovery across different scanning setups. The key finding: architecture and network placement matter more than the scanner itself.</p> <p>Detailed methodology, raw data, and test scripts are in the research article: Nmap vs Masscan vs Rustscan: Myths and Facts</p>"},{"location":"benchmarks/#time-to-scan-1000-targets","title":"Time to scan 1,000 targets","text":"Setup Duration Nmap (home network, default settings) 32h 16m Nmap (home network, tuned settings) 4h 56m Nmap (cloud \u2192 cloud) 1h 12m Distributed, 4 workers 22m 25s Distributed, 10 workers 8m 58s"},{"location":"benchmarks/#observations","title":"Observations","text":"<p>Single-host limits. A single scanner hits network constraints quickly. Running multiple scans on one machine provides negligible gains \u2014 the bottleneck is the network path, not CPU or memory.</p> <p>Network placement. Cloud-to-cloud scanning is roughly 30% faster than scanning from a home connection to cloud targets. The difference comes from lower latency and higher bandwidth between cloud networks.</p> <p>Distributed scaling. Distributing across workers scales nearly linearly. 4 workers are roughly 4x faster than one; 10 workers roughly 10x. The limit is reached when target-side rate limiting or network saturation becomes the bottleneck.</p> <p>Scanner choice. With proper configuration, differences between Nmap, Masscan, and RustScan are marginal for most pentest setups. Configuration and architecture account for the meaningful differences.</p>"},{"location":"benchmarks/#how-this-shaped-falcoria","title":"How this shaped Falcoria","text":"<p>These results drove the architecture:</p> <ul> <li>Distributed execution across independent network paths \u2014 adding machines helps, tuning a single scanner has diminishing returns</li> <li>Predefined scan configs that balance speed and accuracy based on tested conditions</li> <li>Central aggregation into one shared state \u2014 results from all workers merge automatically</li> </ul>"},{"location":"benchmarks/#references","title":"References","text":"<ul> <li>Full research article \u2014 methodology and raw data</li> <li>Cloud-to-cloud benchmarks</li> <li>Home-to-cloud benchmarks</li> <li>Distributed scan benchmarks</li> <li>Extrapolation scripts</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#setup-options","title":"Setup options","text":"<p>There are three ways to run Falcoria, depending on what you need:</p> <p>Single-node \u2014 everything on one machine. ScanLedger, Tasker, Worker, Postgres, Redis, RabbitMQ \u2014 all via Docker Compose. Good for trying Falcoria out or running engagements from one host.</p> <p>Distributed \u2014 components deployed on separate machines. Workers run from different network locations, each with its own IP. This is the setup for large scopes where scan speed matters.</p> <p>Data aggregation only \u2014 just ScanLedger and falcli. No scanning \u2014 you import reports from Nmap or other tools and use ScanLedger to merge and track them.</p>"},{"location":"getting-started/#single-node-setup","title":"Single-node setup","text":"<p>The quickest way to get everything running:</p> <pre><code>git clone https://github.com/Falcoria/falcoria.git\ncd falcoria\n./quickstart.sh\n</code></pre> <p>The script generates TLS certificates, creates credentials, starts all services via Docker Compose, and runs health checks. At the end it prints an admin token \u2014 save it, you'll need it for CLI configuration.</p> <p>Ports exposed: <code>443</code> (ScanLedger API), <code>8443</code> (Tasker API).</p>"},{"location":"getting-started/#installing-falcli","title":"Installing falcli","text":"<p>falcli is the CLI client for interacting with Falcoria. After installing it, edit the profile at <code>./app/data/profiles/default.yaml</code>:</p> <pre><code>backend_base_url: https://&lt;scanledger_host&gt;\ntasker_base_url: https://&lt;tasker_host&gt;\ntoken: &lt;YOUR_ADMIN_TOKEN&gt;\n</code></pre> <p>For single-node setup, both URLs point to <code>localhost</code> with the respective ports.</p> <p>falcli remembers the active project, so you don't need to specify it every time. You can switch between projects with <code>falcli profile set-active-project</code>. Every command supports <code>--help</code> with detailed usage information.</p>"},{"location":"getting-started/#first-scan","title":"First scan","text":""},{"location":"getting-started/#1-create-a-project","title":"1. Create a project","text":"<pre><code>falcli project create --name internal-net\n</code></pre> <p>A project is a persistent dataset. All scans within a project write into the same shared state.</p>"},{"location":"getting-started/#2-start-a-scan","title":"2. Start a scan","text":"<pre><code>falcli scan start --config scan_configs/http-only.yaml --targets-file hosts.txt\n</code></pre> <p>Targets are deduplicated before scanning \u2014 duplicate entries, overlapping CIDRs, hostnames resolving to the same IP are all handled automatically.</p>"},{"location":"getting-started/#3-check-status","title":"3. Check status","text":"<pre><code>falcli project scan status\n</code></pre>"},{"location":"getting-started/#4-view-results","title":"4. View results","text":"<pre><code>falcli project ips get\n</code></pre>"},{"location":"getting-started/#export","title":"5. Export","text":"<pre><code>falcli project ips download\n</code></pre> <p>Exports the current shared state as Nmap XML. This reflects everything across all scans in the project.</p>"},{"location":"getting-started/#importing-external-reports","title":"Importing external reports","text":"<p>If you have existing Nmap XML output, you can import it into ScanLedger without running a scan:</p> <pre><code>falcli project ips import -f scan_report.xml --mode append\n</code></pre> <p>The import mode controls how incoming data merges with what's already stored. See Import Modes for details.</p> <p>This is how ScanLedger works as a standalone tool \u2014 you bring your own scan output, ScanLedger handles aggregation.</p>"},{"location":"getting-started/#distributed-setup","title":"Distributed setup","text":"<p>For distributed scanning, components are deployed separately. Each has its own repository:</p> Component Repository Role ScanLedger Falcoria/scanledger Shared state \u2014 stores and merges all scan data Tasker Falcoria/tasker Target preparation and task distribution Worker Falcoria/worker Scan execution falcli Falcoria/falcli Command-line interface <p>ScanLedger and Tasker run centrally. Workers are deployed on separate machines \u2014 cloud VMs, VPSes, VPN endpoints \u2014 each connecting to the shared RabbitMQ and Redis. Adding workers scales scan throughput linearly.</p> <p>See each repository for installation instructions.</p>"},{"location":"getting-started/#next-steps","title":"Next steps","text":"<ul> <li>Workflows \u2014 common usage patterns across engagements</li> <li>Concepts \u2014 import modes, deduplication, change history, distribution</li> </ul>"},{"location":"workflows/","title":"Workflows","text":"<p>Practical patterns for using Falcoria across common engagement scenarios.</p>"},{"location":"workflows/#expanding-scan-coverage","title":"Expanding scan coverage","text":"<p>Start narrow, then expand. Each phase adds to the existing data without overwriting previous results.</p> <p>Phase 1 \u2014 HTTP ports only, insert mode (default \u2014 adds new hosts, skips known ones):</p> <pre><code>falcli scan start --config http-only.yaml --targets-file hosts.txt\n</code></pre> <p>Phase 2 \u2014 Full port range, append mode (adds new ports, keeps existing results):</p> <pre><code>falcli scan start --config full-range.yaml --mode append --targets-file hosts.txt\n</code></pre> <p>Phase 3 \u2014 Service detection, update mode (refreshes service info on known ports):</p> <pre><code>falcli scan start --config service-detect.yaml --mode update --targets-file hosts.txt\n</code></pre> <p>Each phase uses a different import mode matching its intent. At every point, <code>falcli project ips get</code> returns the full state across all phases and all team members.</p>"},{"location":"workflows/#scope-changes","title":"Scope changes","text":"<p>When the client adds targets or recon finds new hosts, add them to the same target file and re-run:</p> <pre><code>falcli scan start --config http-only.yaml --targets-file hosts.txt\n</code></pre> <p>Falcoria detects which targets are already scanned and sends only the new ones.</p>"},{"location":"workflows/#resuming-interrupted-scans","title":"Resuming interrupted scans","text":"<p>Scans can be stopped at any point:</p> <pre><code>falcli project scan stop\n</code></pre> <p>This clears the queue and stops sending new tasks to workers. Results from targets that were already scanned are kept in ScanLedger.</p> <p>To resume, re-run the same command:</p> <pre><code>falcli scan start --config full-range.yaml --targets-file hosts.txt\n</code></pre> <p>Falcoria checks what's already been scanned and queues only the remaining targets.</p>"},{"location":"workflows/#importing-external-reports","title":"Importing external reports","text":"<p>To consolidate results from tools outside Falcoria (Nmap, Masscan, etc.):</p> <pre><code>falcli project ips import -f nmap_output.xml --mode append\n</code></pre> <p>Choose the import mode based on the report's completeness:</p> <ul> <li>Insert \u2014 adding new hosts with their ports, already known hosts are skipped</li> <li>Append \u2014 adding partial scan results safely</li> <li>Update \u2014 refreshing service info from a follow-up scan</li> <li>Replace \u2014 applying a complete, authoritative scan</li> </ul>"},{"location":"workflows/#distributed-scanning","title":"Distributed scanning","text":"<p>Deploy multiple workers on separate machines. Each worker connects to the same Tasker instance and picks up tasks from the shared queue.</p> <p>All workers send results to the same ScanLedger. No coordination is needed between workers \u2014 the queue handles task assignment, ScanLedger handles merging.</p> <p>To add capacity, deploy additional workers. Throughput scales linearly with worker count until network or target-side limits are reached.</p>"},{"location":"workflows/#port-sharding","title":"Port sharding","text":"<p>To stay under per-source rate limits while covering a large port range, submit multiple scan tasks with different port ranges:</p> <pre><code>falcli scan start --config ports-1-10000.yaml --targets-file hosts.txt\nfalcli scan start --config ports-10001-20000.yaml --targets-file hosts.txt\nfalcli scan start --config ports-20001-65535.yaml --targets-file hosts.txt\n</code></pre> <p>Different workers pick up different tasks and scan from different IPs. Each source stays under the limit; combined, they cover the full range.</p>"},{"location":"workflows/#team-coordination","title":"Team coordination","text":"<p>Multiple team members work against the same Falcoria instance. Each can submit targets, launch scans with different configs, view the current shared state, and check what changed between scans.</p> <p>Duplicates are handled automatically. No manual scope splitting or result collection needed.</p>"},{"location":"workflows/#tracking-changes","title":"Tracking changes","text":"<p>After rescanning, view what changed:</p> <pre><code>falcli project ips history\n</code></pre> <p></p> <p>History shows state transitions: port opened, port closed, service changed, banner updated. On a large scope, this is the fastest way to find what moved between scan cycles.</p>"},{"location":"workflows/#exporting-results","title":"Exporting results","text":"<p>Export the current shared state at any point:</p> <pre><code>falcli project ips download\n</code></pre> <p>The export reflects the current state across all scans, in Nmap XML format. Use this to feed results into other tools or for reporting.</p>"},{"location":"concepts/","title":"Concepts","text":"<p>This section covers the mechanisms behind Falcoria's scan pipeline \u2014 from targets to shared state.</p> <pre><code>Targets\n  \u2193\nDeduplication  \u2014 removes duplicates, resolves hostnames,\n                 skips already-scanned targets by default\n  \u2193\nDistribution   \u2014 splits work across workers\n  \u2193\nWorkers        \u2014 execute scans\n  \u2193\nImport Modes   \u2014 merge results into ScanLedger\n  \u2193\nChange History \u2014 records what changed\n</code></pre> <p>All of the above is configured through a scan config.</p>"},{"location":"concepts/#how-it-all-connects","title":"How it all connects","text":"<p>A single scan command triggers the full pipeline:</p> <ol> <li>Targets go through deduplication \u2014 duplicates are removed, hostnames are resolved and unified by IP, and already-scanned targets are skipped by default</li> <li>Remaining targets are distributed across workers, each scanning from its own IP. The task queue tracks progress, so interrupted scans resume without rescanning</li> <li>Each worker's results are merged into ScanLedger (shared state database) through import modes \u2014 the merge strategy determines whether ports are added, updated, or replaced</li> <li>Change history records what changed since the last run</li> </ol> <p>The result is one shared dataset that stays current without manual file management. The scope state can be exported as Nmap XML or JSON at any point. See Benchmarks for measured examples.</p>"},{"location":"concepts/change-history/","title":"Change History","text":"<p>Change history tracks modifications to port state, service, and banner information between scan runs.</p>"},{"location":"concepts/change-history/#what-is-recorded","title":"What is recorded","text":"<p>A history entry is created when a rescanned port's attributes change:</p> <ul> <li>Port state \u2014 e.g., open \u2192 closed</li> <li>Service \u2014 detected service changed</li> <li>Banner \u2014 application version or banner string updated</li> </ul> <p>Only attributes that were actively rescanned and changed are logged. Ports not included in a scan retain their previous values \u2014 no history entry is created for them.</p>"},{"location":"concepts/change-history/#how-it-works","title":"How it works","text":"<p>When scan results are imported into ScanLedger, incoming data is compared against the existing shared state for the project. For each rescanned target, any detected change is recorded with a timestamp.</p> <p>Unscanned ports and services remain static.</p> <p></p>"},{"location":"concepts/change-history/#why-it-matters","title":"Why it matters","text":"<p>On a large scope scanned repeatedly, most data stays the same between runs. Reviewing full scan output to find what changed is not practical at scale.</p> <p>History surfaces the differences: a new port appeared, a service switched, a banner updated \u2014 without reviewing full scan output.</p>"},{"location":"concepts/change-history/#disabling-history","title":"Disabling history","text":"<p>History tracking can be disabled per scan or import. This is useful when a scan is expected to produce redundant changes \u2014 for example, rescanning with a broader port range where previously found ports will appear again without meaningful differences.</p>"},{"location":"concepts/change-history/#viewing-history","title":"Viewing history","text":"<pre><code>falcli project ips history\n</code></pre> <p>History entries show what changed, when, and on which host/port.</p>"},{"location":"concepts/deduplication/","title":"Deduplication","text":"<p>Deduplication ensures each target is scanned only once. It operates at multiple stages across Tasker and ScanLedger.</p>"},{"location":"concepts/deduplication/#why-it-matters","title":"Why it matters","text":"<p>Target lists often contain duplicates \u2014 the same host under different names, CIDRs overlapping with individual IPs, overlapping scopes from different team members. Without deduplication, the same target gets scanned multiple times, wasting time and adding load on the target network.</p>"},{"location":"concepts/deduplication/#target-deduplication","title":"Target deduplication","text":""},{"location":"concepts/deduplication/#1-string-duplicates","title":"1. String duplicates","text":"<p>Identical entries in the target list are removed. Each entry is normalized for comparison: CIDRs are expanded to their first host address, IPs are canonicalized (leading zeros removed), hostnames are compared as-is.</p>"},{"location":"concepts/deduplication/#2-resolution-and-ip-level-unification","title":"2. Resolution and IP-level unification","text":"<p>After string deduplication, all entries are resolved into IP addresses:</p> <ul> <li>CIDRs are expanded into individual host IPs</li> <li>Hostnames are resolved via DNS (IPv4, with retries on failure)</li> <li>When multiple hostnames resolve to the same IP, they are collapsed into a single target. The resolved IP becomes the key, all originating hostnames are kept as metadata.</li> </ul> <p>If <code>single_resolve</code> is enabled, only the first resolved IP per hostname is used \u2014 useful for CDN cases where one hostname resolves to many IPs.</p> <p></p>"},{"location":"concepts/deduplication/#system-level-deduplication","title":"System-level deduplication","text":""},{"location":"concepts/deduplication/#3-skipping-already-stored-targets","title":"3. Skipping already stored targets","text":"<p>Targets already present in ScanLedger for the current project are excluded from scanning. This includes hostnames that resolve to an already known IP. If an excluded target carried new hostnames, those hostnames are still merged into the existing record without triggering a new scan.</p> <p>This applies when using Insert mode. Other modes rescan existing targets to update their data.</p>"},{"location":"concepts/deduplication/#4-queue-deduplication","title":"4. Queue deduplication","text":"<p>Targets already queued for the current project are skipped. This is checked both before dispatching and immediately before sending each task, to handle race conditions between concurrent scan requests.</p>"},{"location":"concepts/deduplication/#effect","title":"Effect","text":"<p>The result: raw target lists with duplicates, overlapping ranges, and mixed hostnames are reduced to a set of unique targets that haven't been scanned yet. Team members can submit overlapping scopes without coordination \u2014 duplicates are filtered automatically.</p>"},{"location":"concepts/distribution/","title":"Distribution","text":"<p>Distribution splits scan tasks across multiple workers running on separate machines.</p>"},{"location":"concepts/distribution/#when-it-helps","title":"When it helps","text":"<p>Large scopes \u2014 scanning hundreds or thousands of targets on a single machine takes hours or days. Distributing across workers reduces total scan time proportionally.</p> <p>Dynamic scopes \u2014 when the scope changes regularly, faster scanning allows frequent rescans to track changes.</p>"},{"location":"concepts/distribution/#how-it-works","title":"How it works","text":"<p>Workers consume tasks from a shared queue independently. Each worker runs on its own machine with its own network path and IP address. See Architecture for how the components fit together.</p> <p></p>"},{"location":"concepts/distribution/#why-its-effective","title":"Why it's effective","text":"<p>The bottleneck in scanning is the network path, not the scanner itself. A single machine is limited by its bandwidth and rate limits imposed by the target network.</p> <p>Multiple workers bypass this by scanning from independent network paths. Each worker stays within rate limits individually; combined, they cover the scope faster.</p> <p>Throughput scales roughly linearly: 4 workers are roughly 4x faster, 10 workers roughly 10x. The limit is reached when target-side rate limiting or network saturation becomes the bottleneck.</p> <p>See Benchmarks for measured performance data.</p>"},{"location":"concepts/import-modes/","title":"Import Modes","text":"<p>Import modes control how scan results are merged into ScanLedger \u2014 whether to add ports, update service info, or mark ports as closed.</p>"},{"location":"concepts/import-modes/#ip-level-behavior","title":"IP-level behavior","text":"<p>All modes share the same IP-level logic:</p> <ul> <li>New IPs from the report are always added.</li> <li>Existing IPs are never removed.</li> </ul> <p>The difference between modes is in how they handle ports.</p>"},{"location":"concepts/import-modes/#port-level-reference","title":"Port-level reference","text":"<p>How existing data and an incoming report are combined for a single port. INSERT is not in the table \u2014 it operates on whole hosts, not individual ports.</p> ScanLedger (before) Incoming report APPEND UPDATE REPLACE 22/tcp open OpenSSH 22/tcp open \u26aa unchanged \u26aa unchanged \ud83d\udfe1 updated 22/tcp open 22/tcp open OpenSSH \u26aa unchanged \ud83d\udfe1 updated \ud83d\udfe1 updated 22/tcp open 22/tcp closed \u26aa unchanged \u26aa unchanged \ud83d\udd34 closed \u2014 22/tcp open OpenSSH \ud83d\udfe2 added \ud83d\udfe2 added \ud83d\udfe2 added 22/tcp open \u2014 (port in scanned range) \u26aa unchanged \u26aa unchanged \ud83d\udd34 closed <ul> <li>\ud83d\udfe2 added \u2014 port was created</li> <li>\ud83d\udfe1 updated \u2014 existing port was modified</li> <li>\u26aa unchanged \u2014 existing port was not touched</li> <li>\ud83d\udd34 closed \u2014 port state changed to closed</li> </ul>"},{"location":"concepts/import-modes/#insert","title":"Insert","text":"<ul> <li>New hosts are added with their ports.</li> <li>Existing hosts are skipped entirely.</li> </ul>"},{"location":"concepts/import-modes/#append","title":"Append","text":"<ul> <li>New ports are added.</li> <li>Existing ports are not modified.</li> <li>Ports are never closed.</li> </ul>"},{"location":"concepts/import-modes/#update","title":"Update","text":"<ul> <li>New ports are added.</li> <li>Existing ports are updated if the report has new data (service, banner).</li> <li>Empty fields in the report do not overwrite existing data.</li> <li>Ports are never closed.</li> </ul>"},{"location":"concepts/import-modes/#replace","title":"Replace","text":"<ul> <li>New ports are added.</li> <li>Existing ports are updated if the report has new data (service, banner).</li> <li>Empty fields in the report do not overwrite existing data.</li> <li>Ports reported as closed or filtered are marked as closed.</li> <li>Ports absent from the report but within the scanned range are also marked as closed. Falcoria parses the scan command line to determine which ports were in scope \u2014 if a known port falls within that range but has no result, it is treated as closed.</li> </ul>"},{"location":"concepts/import-modes/#when-to-use-each-mode","title":"When to use each mode","text":"<ul> <li>Insert \u2014 merging host lists from different sources, importing repeated scans of the same scope, aggregating recon output.</li> <li>Append \u2014 partial or fast scans where some ports may be missed. Adds what was found without overwriting existing data.</li> <li>Update \u2014 follow-up service detection scans. Refreshes service info on known ports while protecting against false closures.</li> <li>Replace \u2014 complete, high-confidence scans where results should be applied as-is.</li> </ul>"},{"location":"concepts/import-modes/#specifying-a-mode","title":"Specifying a mode","text":"<p>When scanning:</p> <pre><code>falcli scan start --config &lt;config&gt; --mode append --targets-file hosts.txt\n</code></pre> <p>When importing:</p> <pre><code>falcli project ips import -f report.xml --mode replace\n</code></pre>"},{"location":"concepts/scan-configs/","title":"Scan Configs","text":"<p>A scan config is a YAML file that defines how a scan is executed \u2014 which ports to scan, which protocols to use, and how results are processed.</p>"},{"location":"concepts/scan-configs/#separation-of-concerns","title":"Separation of concerns","text":"<p>Targets define what is scanned. Configs define how it is scanned. This separation allows the same target list to be scanned with different configurations across phases \u2014 HTTP-only first, then full range, then service detection \u2014 without duplicating target management.</p>"},{"location":"concepts/scan-configs/#two-phase-scanning","title":"Two-phase scanning","text":"<p>Each scan has up to two phases:</p> <ol> <li> <p>Port discovery (<code>open_ports_opts</code>) \u2014 finds which ports are open. Configurable: port range, protocol (TCP/UDP), rate limits, retries, timeouts.</p> </li> <li> <p>Service detection (<code>service_opts</code>) \u2014 runs against discovered open ports only, if <code>include_services: true</code>. Configurable: aggressive scan, default scripts, OS detection.</p> </li> </ol> <p>This avoids probing closed ports for service information. Both phases are configured independently within the same YAML file.</p>"},{"location":"concepts/scan-configs/#config-reference","title":"Config reference","text":"<p>A complete example with all available options and their descriptions is available at <code>all_options_example.yaml</code>.</p>"},{"location":"concepts/scan-configs/#built-in-profiles","title":"Built-in profiles","text":"<p>Falcoria ships with predefined configs for common scan types. Available profiles are in the <code>scan_configs</code> directory. You can modify existing profiles or create new ones \u2014 they are plain YAML files, portable and reusable across projects and team members.</p>"},{"location":"concepts/scan-configs/#staged-scanning","title":"Staged scanning","text":"<p>Configs enable a phased approach \u2014 start with HTTP ports, expand to full range, then run service detection. Each phase uses a different config against the same targets. See Workflows for a practical example.</p>"},{"location":"concepts/scope-state/","title":"Scope state","text":"<p>Scope state is the current view of all hosts, ports, and services in a project \u2014 built from every scan applied through import modes. A single scan updates it, but the scope state reflects all scans combined.</p> <p>The state is shared across the team. One project has one scope state, accessible to every team member through the API or falcli. When someone runs a scan or imports a report, the scope state updates for everyone.</p> <p>Changes between scans \u2014 ports opening, closing, services updating \u2014 are recorded separately in change history.</p>"},{"location":"concepts/scope-state/#data-model","title":"Data model","text":"<p>IP is the primary entity in the data model.</p> <pre><code>Project\n  \u251c\u2500\u2500 IP\n  \u2502     \u251c\u2500\u2500 Ports\n  \u2502     \u2502     \u2514\u2500\u2500 Service info\n  \u2502     \u2514\u2500\u2500 Hostnames (many-to-many)\n  \u2514\u2500\u2500 Hostname\n        \u2514\u2500\u2500 IPs (many-to-many)\n</code></pre> <ul> <li>Each IP is unique per project. Each port is unique per IP, protocol, and port number.</li> <li>Hostnames are linked to IPs through a many-to-many relationship: one hostname can resolve to multiple IPs, and one IP can have multiple hostnames. This is why deduplication resolves hostnames before scanning \u2014 to avoid scanning the same IP twice under different names.</li> <li>Everything is scoped to a project. Different projects have independent scope states.</li> </ul>"},{"location":"concepts/scope-state/#how-scope-state-gets-updated","title":"How scope state gets updated","text":"<p>Each scan is applied through an import mode that determines what happens to existing data:</p> <ul> <li>New IPs and ports are added</li> <li>Existing service info can be refreshed or left unchanged</li> <li>Ports can be marked as closed (in replace mode)</li> </ul> <p>The system parses the original scan command to know which ports were in scope \u2014 so it can distinguish \"this port was not scanned\" from \"this port was scanned and found closed\".</p>"},{"location":"concepts/scope-state/#scope-state-vs-scan-report","title":"Scope state vs scan report","text":"<p>Scope state is built from all applied scans. It reflects which hosts are known, which ports are open, what services are running. When a new scan is applied, the scope state gets updated accordingly.</p> <p>A scan report is a file from one scan at one point in time. It exists on its own and has no context about previous scans.</p> <p>The scope state can be exported as Nmap XML or JSON at any time \u2014 a snapshot of whatever the current state is.</p>"}]}